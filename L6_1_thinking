Thinking1：GBDT与XGBoost的区别是什么？
答：GBDT全称为gradient boosting decision tree，意思为梯度提升决策树，它是以cart树为基础、使用boosting原理的集成学习算法；而xgboost是GBDT的工程版本，在其原有的基础上增加了许多扩展和改进：
    1）GBDT以CART作为基分类器，xgboost还支持线性分类器，如逻辑斯蒂回归（分类问题）或线性回归（回归问题）；
    2）GBDT在优化时只用到一阶导数，即gradien梯度，而xgboost则对损失函数进行了二阶泰勒展开，同时用到了一阶和二阶导数（好像是牛顿法）；
    3）xgboost增加了正则化项，考虑了树的复杂度，即树的叶子节点个数、每个叶子节点权重（节点的输出值）的L2模的平方和；
    4）xgboost在寻找最佳分割点时，采用近似贪心算法，提高了计算速度；
    5）xgboost支持“并行”计算，这种“并行”并不是树和树之间并行生成，而是特征计算的并行，模型会预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量，并且
      在进行节点分裂时，计算每个特征的增益，选择增益最大的特征作为分割节点，各个特征的增益计算可以使用多线程并行；
    6）xgboost还使用了近似直方图算法，用于高效地生成候选的分割点。
    
Thinking2：举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何... 请分享到课程微信群中）
答：一个kaggel例子：
    1）背景：银行个人贷款，预测贷款申请人的偿还结果 -> 二分类预测问题
    2）问题：使得贷款风险最小 -> 还款率最高
    3）模型：catboost，申请人数据来源为贷款申请表，数据基本上为类别数据
    4）效果：使用auc、F1 score和混淆矩阵作为模型评价指标，但是考虑到模型目标为风险最小，因此应着重关注precision指标，通过绘制pr曲线，调整分类结果的阈值，找到使得tpr最高的阈值，以此用于实际预测，
       最终的tpr为78%。

Thinking3：请你思考，在你的工作中，需要构建哪些特征（比如用户画像，item特征...），这些特征都包括哪些维度（鼓励分享到微信群中，进行交流）
答：（只能粗略说说，怕泄密被开除）工作针对内部业务，分析员工完成任务的情况，包括任务数量、质量、时效、人效等等，也可以借鉴推荐系统的思想，构建员工特征和任务特征，根据任务完成评价指标给员工
    推荐任务，而非现在的从任务库中随机分发任务（也可能是按任务队列顺序发放，没深入了解过）。这里的特征构建：
    1）员工特征，借鉴“人口·消费·行为·分析”构建，人口特征除了包含基本的人口统计学特征外，还应包括与任务相关的经验、学习专业、兴趣；消费特征这里就是任务执行特征，包括执行习惯、执行力等；
       行为特征就是员工执行任务的耗时、休息过度时长等隐形特征；分析特征主要就是任务的类别属性；
    2）任务特征，即item特征，物理属性、被完成的质量、耗时等
    这里也会有冷启动问题，如新员工入职的情况，或新任务出现的情况；但最大的差异或困难是每个任务一般只会被完成一次（有检验会多1-2次），不像课程例子中多个用户会给同一个item打分或购买同一个商品，
    员工行为上缺少真正的共同点，因此任务相似度的描述和计算上面要更深入考虑。
